**ğŸ“˜ Automated Answer Sheet Evaluation System**

An AI-powered platform for evaluating student answer sheets automatically with NLP + ML + LLM support.

**ğŸš€ Overview**

The Automated Answer Sheet Evaluation System is an AI-based solution that evaluates subjective and objective answers submitted by students.
It eliminates manual checking, improves accuracy, and provides instant feedback using NLP, Machine Learning, and LLM models.

This project supports:

Automated subjective answer scoring

Keyword matching & semantic similarity

LLM-based evaluation (optional)

Mis-spell correction

Detailed feedback generation

Marks calculation

Dashboard for students & teachers

**ğŸ¯ Key Features**

OCR Support â€“ Extract answers from scanned sheets

Keyword-based Scoring

Semantic Similarity Scoring (Sentence Transformers)

LLM-based Evaluation (GPT/Llama/Gemma supported)

Automated Marks Calculation

Feedback Generator

Plagiarism Detection (Optional)

Web Interface (FastAPI / Streamlit)

Docker Support

MongoDB Logging

**ğŸ§± Architecture**
Input Sheet â†’ OCR â†’ Text Extraction â†’ Preprocessing â†’ 
Model Evaluation (ML + LLM) â†’ Scoring Engine â†’ Feedback â†’ Dashboard

ğŸ› ï¸ Tech Stack

Backend:

Python

FastAPI / Flask

Sentence Transformers

OpenAI / HuggingFace LLMs

NLTK / SpaCy

Frontend:

Streamlit / React (optional)

Database:

MongoDB (store answers, logs, scores)

Redis (cache model outputs)

Deployment:

Docker

Docker Compose

**ğŸ“‚ Project Structure**
project/
â”‚â”€â”€ app.py                  # Main API
â”‚â”€â”€ evaluator.py            # Scoring engine
â”‚â”€â”€ llm_model.py            # LLM evaluation wrapper
â”‚â”€â”€ ocr_module.py           # OCR processing
â”‚â”€â”€ utils/                  # NLP utils
â”‚â”€â”€ test_samples/           # Sample answer sheets
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ Dockerfile
â”‚â”€â”€ README.md

**ğŸ§ª How It Works**
1. Upload Answer Sheet

Accepts images (.jpg, .png) or PDFs

2. OCR Processing

Extracts text using Tesseract or EasyOCR

3. Preprocessing

Sentence cleaning

Stopword removal

Lemmatization

4. Evaluation Engine

Uses a mix of:

Method	Purpose
Keyword Matching	Basic scoring
Cosine Similarity	Semantic meaning match
LLM Evaluation	Open-ended subjective answer scoring
Rule-based checking	Must-include points
5. Score Calculation

Final score = weighted sum of all evaluation results.

6. Feedback Generation

LLM dynamically generates feedback for the student.

**ğŸ§° Installation**
1ï¸âƒ£ Clone the Repository
git clone https://github.com/your-username/answer-evaluation.git
cd answer-evaluation

2ï¸âƒ£ Create Virtual Environment
python -m venv venv
.\venv\Scripts\activate

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

4ï¸âƒ£ Add LLM API Key

Create .env file:

OPENAI_API_KEY=your_key_here

ğŸ³ Run with Docker
Build Image:
docker build -t answer-evaluator .

Run Container:
docker run -p 8000:8000 answer-evaluator

â–¶ï¸ Run the Application (Without Docker)
uvicorn app:app --reload


Open in browser:

http://localhost:8000/docs

**ğŸ§ª API Endpoints**
Endpoint	Method	Description
/evaluate	POST	Upload answer sheet / text to evaluate
/feedback	GET	Get AI-generated feedback
/score	GET	Fetch score for student
/health	GET	Check API health
ğŸ“Š Sample Response
{
  "question_id": "Q1",
  "score": 8.5,
  "max_score": 10,
  "similarity": 0.82,
  "keywords_matched": ["ecosystem", "interaction"],
  "feedback": "Good answer! You covered the key points but can include more examples."
}

ğŸ§© Future Enhancements

Add multi-language support

Add handwriting recognition

Train custom evaluation model

Integrate with college ERP

Add grader dashboard

ğŸ‘¨â€ğŸ’» Contributors

Your Name

Team Members
